{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10091196,"sourceType":"datasetVersion","datasetId":6222563},{"sourceId":10102472,"sourceType":"datasetVersion","datasetId":6231230},{"sourceId":10366163,"sourceType":"datasetVersion","datasetId":6420590}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport nltk\n\n\ndf = pd.read_json(\"/kaggle/input/f1-stats/bert_dataset.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('punkt')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[[\"text\", \"sentiment\"]]\n# label_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n# df[\"sentiment\"] = df[\"sentiment\"].map(label_mapping)\nstop_words = set(stopwords.words('english'))\ndf.head(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text_nltk(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    text = text.lower()\n    words = word_tokenize(text)\n    words = [word for word in words if word not in stop_words]\n    return ' '.join(words)\n\ndf['text'] = df['text'].apply(clean_text_nltk)\nprint(\"done\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df[\"text\"]\ny = to_categorical(df['sentiment'], num_classes=3)\nprint(\"done\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Tokenize and pad text data\ntokenizer = Tokenizer(num_words=100000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_val_seq = tokenizer.texts_to_sequences(X_val)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n\nX_train_pad = pad_sequences(X_train_seq, padding='post')  # Adjust maxlen as needed\nX_val_pad = pad_sequences(X_val_seq, padding='post')\nX_test_pad = pad_sequences(X_test_seq, padding='post')\n\nfull_X1 = tokenizer.texts_to_sequences(X)\nfull_X = pad_sequences(full_X1, padding='post')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(X_train_pad))\nprint(len(X_val_pad))\nprint(len(X_test_pad))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model_acc(model):\n    \n    loss, accuracy = model.evaluate(X_test_pad, y_test)\n    print(f\"Test Accuracy: {accuracy:.2f}\")\n    \n    predictions = model.predict(X_test_pad)\n    predicted_labels = predictions.argmax(axis=1)  # Convert probabilities to label indices\n    return predicted_labels\n\nimport matplotlib.pyplot as plt\ndef plot_results(history, model_name: str):\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n    plt.legend()\n    plt.title(\"Model Accuracy\")\n    plt.savefig(f'{model_name}_acc.png')\n    plt.show()\n    \n    # Plot training and validation loss\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Val Loss')\n    plt.legend()\n    plt.title(\"Model Loss\")\n    plt.savefig(f'{model_name}_loss.png')\n    plt.show()\n    \nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(predicted_labels, model_name):\n    inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n    cm = confusion_matrix(y_test.argmax(axis=1), predicted_labels)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=inverse_label_mapping.values(), yticklabels=inverse_label_mapping.values())\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.savefig(f\"{model_name}_confusion_matrix.png\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional, SpatialDropout1D, LayerNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2, l1\n\nmodel_gru = Sequential([\n    Embedding(input_dim=30000, output_dim=128),\n    SpatialDropout1D(0.3),\n    Bidirectional(GRU(units=64, return_sequences=True, kernel_regularizer=l2(0.01))),  \n    LayerNormalization(),\n    Dropout(0.3),  # Increased dropout\n    Bidirectional(GRU(units=32, kernel_regularizer=l2(0.01))),\n    LayerNormalization(),\n    Dropout(0.3),  # Increased dropout\n    Dense(units=3, activation='softmax', kernel_regularizer=l2(0.01))  \n])\n\n# Compile the model\nmodel_gru.compile(optimizer=Adam(learning_rate=2e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel_gru.summary()\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n# batch_logger = BatchMetricsLogger()\n\n# history_gru = model_gru.fit(\n#     X_train_pad, y_train,\n#     validation_data=(X_val_pad, y_val),\n#     epochs=80,  # Adjust as needed\n#     batch_size=128,\n#     callbacks=[early_stopping]  # Optional early stopping\n# )\n# predictions = model_gru.predict(full_X)\n# predicted_labels = predictions.argmax(axis=1)\n# df[\"gru\"] = predicted_labels\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nword2vec_path = '/kaggle/input/ward2vecc/GoogleNews-vectors-negative300.bin'\nword2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional, SpatialDropout1D, LayerNormalization, Conv1D, GlobalMaxPooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2, l1\n\nmodel_cnn = Sequential([\n    Embedding(input_dim=30_000, output_dim=128),\n    SpatialDropout1D(0.5),\n    Conv1D(filters=128, kernel_size=3, activation='relu'),\n    Dropout(0.3),\n    Conv1D(filters=128, kernel_size=3, activation='relu'),\n    Dropout(0.3),\n    Conv1D(filters=128, kernel_size=3, activation='relu'),\n    Dropout(0.3), \n    GlobalMaxPooling1D(),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(3, activation='softmax')  # 3 classes: 0, 1, 2\n])\nmodel_cnn.compile(optimizer= Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_cnn.summary()\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n# batch_logger = BatchMetricsLogger()\n\n# history_cnn = model_cnn.fit(\n#     X_train_pad, y_train,\n#     validation_data=(X_val_pad, y_val),\n#     epochs=80,  # Adjust as needed\n#     batch_size=128,\n#     callbacks=[early_stopping]  # Optional early stopping\n# )\n\n# predictions = model_cnn.predict(full_X)\n# predicted_labels = predictions.argmax(axis=1)\n# df[\"cnn\"] = predicted_labels\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Define parameters\nvocab_size = 30000  # Same as the input_dim in your Embedding layer\nembedding_dim = 300  # Same as the output_dim in your Embedding layer\n\n# Initialize embedding matrix\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n# Create a word-to-index dictionary for your tokenizer\nword_index = tokenizer.word_index  # Assuming you already have a tokenizer\n\n# Fill the embedding matrix with Word2Vec vectors\nfor word, i in word_index.items():\n    if i < vocab_size:\n        try:\n            embedding_vector = word2vec[word]\n            embedding_matrix[i] = embedding_vector\n        except KeyError:\n            pass  # Word not in Word2Vec vocabulary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_layer = Embedding(\n    input_dim=vocab_size, \n    output_dim=embedding_dim, \n    weights=[embedding_matrix], \n    trainable=True  # Set to False if you don't want to fine-tune embeddings\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional, SpatialDropout1D, LayerNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2, l1\n\nmodel_gru_w = Sequential([\n    Embedding(input_dim=30_000, output_dim=64),\n    SpatialDropout1D(0.1),\n    Bidirectional(GRU(units=64, return_sequences=True, kernel_regularizer=l2(0.01))),  \n    LayerNormalization(),\n    Dropout(0.25),  \n    Bidirectional(GRU(units=32, kernel_regularizer=l2(0.01))),\n    LayerNormalization(),\n    Dropout(0.25),\n    Dense(units=3, activation='softmax', kernel_regularizer=l2(0.01))  \n])\n\n# Compile the model\nmodel_gru_w.compile(optimizer=Adam(learning_rate=2e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel_gru_w.summary()\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n# batch_logger = BatchMetricsLogger()\n\n# history_gruw = model_gru_w.fit(\n#     X_train_pad, y_train,\n#     validation_data=(X_val_pad, y_val),\n#     epochs=80,  # Adjust as needed\n#     batch_size=128,\n#     callbacks=[early_stopping]  # Optional early stopping\n# )\n\n# predictions = model_gru_w.predict(full_X)\n# predicted_labels = predictions.argmax(axis=1)\n# df[\"gru_word2vec\"] = predicted_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n\nmodel_cnnw = Sequential([\n    embedding_layer,\n    SpatialDropout1D(0.1),\n    Conv1D(filters=128, kernel_size=5, activation='relu'),\n    Dropout(0.3),\n    Conv1D(filters=128, kernel_size=5, activation='relu'),\n    Dropout(0.3),\n    Conv1D(filters=128, kernel_size=5, activation='relu'),\n    Dropout(0.3), \n    GlobalMaxPooling1D(),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(3, activation='softmax')  # 3 classes: 0, 1, 2\n])\nmodel_cnnw.compile(optimizer= Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_cnnw.summary()\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n# batch_logger = BatchMetricsLogger()\n\n# history_cnnw = model_cnnw.fit(\n#     X_train_pad, y_train,\n#     validation_data=(X_val_pad, y_val),\n#     epochs=80,  # Adjust as needed\n#     batch_size=128,\n#     callbacks=[early_stopping]  # Optional early stopping\n# )\n\n# predictions = model_cnnw.predict(full_X)\n# predicted_labels = predictions.argmax(axis=1)\n# df[\"cnn_word2vec\"] = predicted_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    Embedding, SpatialDropout1D, Conv1D, MaxPooling1D,\n    Bidirectional, GRU, LayerNormalization, Dropout, Dense\n)\nfrom tensorflow.keras.regularizers import l2\n\nmodel_gru_cnn = Sequential([\n    embedding_layer,  \n    \n    SpatialDropout1D(0.1),\n    \n    Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n    MaxPooling1D(pool_size=2),\n    \n    Bidirectional(GRU(units=64, return_sequences=True, kernel_regularizer=l2(0.01))),\n    LayerNormalization(),\n    Dropout(0.3),\n    \n    Bidirectional(GRU(units=32, kernel_regularizer=l2(0.01))),\n    LayerNormalization(),\n    Dropout(0.3),\n    \n    Dense(units=3, activation='softmax', kernel_regularizer=l2(0.01))\n])\n\nmodel_gru_cnn.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel_gru_cnn.summary()\n\nfrom tensorflow.keras.utils import plot_model\n\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n\nhistory_gru_cnn = model_gru_cnn.fit(\n    X_train_pad, y_train,\n    validation_data=(X_val_pad, y_val),\n    epochs=80,  # Adjust as needed\n    batch_size=128,\n    callbacks=[early_stopping]  # Optional early stopping\n)\n\npredictions = model_gru_cnn.predict(full_X)\npredicted_labels = predictions.argmax(axis=1)\ndf[\"gru_cnn\"] = predicted_labels\nmodel_gru_cnn.save(\"model_gru_cnn.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_gru_cnn.save(\"model_gru_cnn.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.to_json(\"final_dataset.json\",orient=\"records\", lines=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, BertTokenizer, BertForSequenceClassification\n\nimport tensorflow as tf\n\ntemp_texts, test_texts, temp_labels, test_labels = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    temp_texts, temp_labels, test_size=0.2, random_state=42\n)\nprint(len(train_texts))\nprint(len(val_texts))\nprint(len(test_texts))\nfrom transformers import DistilBertTokenizer\nfrom torch.utils.data import Dataset, DataLoader\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(pd.DataFrame({\n    \"text\": train_texts.tolist(),\n    \"label\": train_labels.tolist(),\n}))\nval_dataset = Dataset.from_pandas(pd.DataFrame({\n    \"text\": val_texts.tolist(),\n    \"label\": val_labels.tolist(),\n}))\n\ntest_dataset = Dataset.from_pandas(pd.DataFrame({\n    \"text\": test_texts.tolist(),\n    \"label\": test_labels.astype(int).tolist(), \n}))\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)\ntest_dataset = test_dataset.map(preprocess_function, batched=True)\n\ntrain_dataset = train_dataset.remove_columns([\"text\"])\nval_dataset = val_dataset.remove_columns([\"text\"])\ntest_dataset = test_dataset.remove_columns([\"text\"])\n\ntrain_dataset.set_format(\"torch\")\nval_dataset.set_format(\"torch\")\ntest_dataset.set_format(\"torch\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results6\",\n    num_train_epochs=8,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    gradient_accumulation_steps = 2,\n    warmup_steps=500,\n    lr_scheduler_type = \"linear\",\n    weight_decay = 0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs4\",\n    logging_steps=100,\n    learning_rate=2e-5,\n    load_best_model_at_end=True,\n    fp16=True, \n    overwrite_output_dir=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    \n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"weighted\")\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"YOUR TOKEN\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./saved_model\")\ntokenizer.save_pretrained(\"./saved_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_results = trainer.evaluate(eval_dataset=test_dataset)\n\nprint(\"\\nTest Set Metrics:\")\nfor key, value in test_results.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions = trainer.predict(test_dataset)\ntest_logits = test_predictions.predictions\ntest_labels = test_predictions.label_ids\ntest_preds = np.argmax(test_logits, axis=-1)\n\nfrom sklearn.metrics import confusion_matrix, classification_report\ncm = confusion_matrix(test_labels, test_preds)\nprint(\"\\nTest Set Confusion Matrix:\\n\", cm)\n\nprint(\"\\nTest Set Classification Report:\\n\", classification_report(test_labels, test_preds))\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ninverse_label_mapping = {v: k for k, v in label_mapping.items()}\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=inverse_label_mapping.values(), yticklabels=inverse_label_mapping.values())\nplt.savefig(\"bert_confusion_matrix.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nlog_history = trainer.state.log_history\n\n# Extract loss and accuracy for plotting\ntrain_loss = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]\neval_loss = [entry[\"eval_loss\"] for entry in log_history if \"eval_loss\" in entry]\neval_acc = [entry[\"eval_accuracy\"] for entry in log_history if \"eval_accuracy\" in entry]\n\nplt.figure(figsize=(12, 5))\nplt.plot(train_loss, label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.legend()\nplt.savefig(\"bert_train_loss.png\")\nplt.show()\n\nplt.figure(figsize=(12, 5))\nplt.plot(eval_loss, label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Validation Loss\")\nplt.legend()\nplt.savefig(\"bert_val_loss.png\")\nplt.show()\n\nplt.figure(figsize=(12, 5))\nplt.plot(eval_acc, label=\"Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Validation Accuracy Over Epochs\")\nplt.legend()\nplt.savefig(\"bert_val_acc.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nmodel = trainer.model\n\ndevice = torch.device(\"cuda\")\nmodel.to(device)\n\ndef predict_sentiment(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    \n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    logits = outputs.logits\n    sentiment_score = torch.softmax(logits, dim=-1)\n    \n    predicted_class = torch.argmax(sentiment_score, dim=-1).item()\n    return predicted_class\n\ndf['bert'] = df['text'].apply(predict_sentiment)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}